# Self-Instruct Data Generation Project

This project aims to generate a large dataset of instructions using the Self-Instruct methodology with the Llama-3 model. The primary goal is to bootstrap a small set of manually-written instructions into a comprehensive dataset through iterative generation, filtering, and refinement processes.

## Project Files

### Python Scripts

#### bootstrap_instructions.py

This script generates new instructions using a language model (Llama-3). It takes a set of seed instructions and produces a specified number of new instructions.

**Key Functions:**
- `encode_prompt()`: Encodes multiple prompt instructions into a single string.
- `sample_machine_instructions()`: Samples machine-generated instructions.
- `post_process_llama3_response()`: Processes the responses from Llama-3.
- `parse_args()`: Parses command-line arguments.

**Main execution:** Loads seed instructions, generates new instructions, and saves them to a JSONL file.

#### gpt3_api.py

This script contains functions to interact with the GPT-3 API. Although adapted for GPT-3, it has been modified to work with Llama-3 in this project.

**Key Function:**
- `make_requests()`: Sends requests to the Llama-3 API and retrieves responses.

#### prepare_for_finetuning.py

This script prepares the generated instructions for fine-tuning a language model. It processes and filters the instances and saves the cleaned data.

**Key Functions:**
- `encode_instance()`: Encodes instruction, input, and output into a specific format.
- `filter_duplicate_instances()`: Filters out duplicate instances.
- `filter_invalid_instances()`: Filters out invalid instances.
- `parse_instances_for_generation_task()`: Parses instances for generation tasks.
- `parse_instances_for_classification_task()`: Parses instances for classification tasks.

**Main execution:** Loads generated tasks, processes them, and saves the cleaned instances for fine-tuning.

### Shell Scripts

#### generate_instructions.sh

A shell script to run `bootstrap_instructions.py` with specified parameters.

**Usage:**
```sh
bash generate_instructions.sh
```
prepare_for_finetuning.sh
A shell script to run prepare_for_finetuning.py with specified parameters.

**Usage**

```sh
bash prepare_for_finetuning.sh
```
## Setup and Usage

1. **Get a Groq API Key**
   - Sign up at Groq to get a free API key.

2. **Create a .env File**
   - Create a `.env` file in the root directory of the project and add your Groq API key:
     ```makefile
     GROQ_API_KEY=your_api_key_here
     ```

3. **Install Dependencies**
   - Make sure you have Python installed. Install the required Python packages using:
     ```sh
     pip install -r requirements.txt
     ```

4. **Prepare Seed Tasks**
   - Ensure you have your seed tasks ready in the `data/seed_tasks.jsonl` file. This file contains the initial set of instructions to bootstrap from.

5. **Generate Instructions**
   - Run the `generate_instructions.sh` script to generate new instructions:
     ```sh
     bash generate_instructions.sh
     ```

6. **Prepare Data for Fine-Tuning**
   - Run the `prepare_for_finetuning.sh` script to prepare the generated instructions for fine-tuning:
     ```sh
     bash prepare_for_finetuning.sh
     ```

# Understanding the Output i.e. Machine-Generated JSONL File

This README provides an overview and explanation of the machine-generated JSONL file produced by the script.

## Overview

The JSONL (JSON Lines) file contains a list of JSON objects, each representing a single instruction generated by the Llama-3 model. Each JSON object consists of several key components which are explained in detail below.

## JSON Object Structure

Each JSON object in the file includes the following fields:

1. **instruction**:
   - The newly generated instruction by the Llama-3 model.

2. **most_similar**:
   - A dictionary where the keys are existing instructions and the values are the ROUGE-L similarity scores between the generated instruction and these existing instructions. This field indicates which existing instructions are most similar to the newly generated one.

3. **avg_similarity_score**:
   - The average similarity score of the generated instruction compared to the most similar instructions. This provides a single metric indicating how similar the new instruction is to the existing ones.

4. **metadata**:
   - Additional details about the generation process:
     - **prompt**: The prompt used to generate the instruction. It typically includes a series of tasks or instructions.
     - **response**: The raw response from the Llama-3 model.
     - **choices**: An array of choices generated by the model. Each choice contains a text field with the generated instruction.
     - **created_at**: The timestamp when the instruction was generated.
     - **request_idx**: The index of the request that generated this instruction.

## Example Breakdown

Below is an example of one of the entries in the JSONL file, followed by a detailed explanation:

```json
{
  "instruction": "Generate a one-sentence description for each of the following people: * historical figure: cleopatra + cleopatra was the last active pharaoh of ancient egypt, known for her beauty, intelligence, and diplomatic skills. * fictional character: sherlock holmes + sherlock holmes is a brilliant detective created by sir arthur conan doyle, famous for his analytical mind and ability to solve complex mysteries. * real person: elon musk + elon musk is a pioneering entrepreneur and business magnate, known for his innovative ventures in electric cars, space exploration, and renewable energy. * ancient mythological figure: persephone + persephone was the queen of the underworld in greek mythology, abducted by hades and forced to spend part of each year with him, causing seasons to change.",
  "most_similar": {
    "Generate a one-sentence description for each of the following people. * historical figure: cleopatra - the last active pharaoh of ancient egypt, known for her beauty, intelligence, and diplomatic skills. * fictional character: sherlock holmes - a brilliant and eccentric detective renowned for his exceptional analytical mind and ability to solve complex cases. * real person: elon musk - a pioneering entrepreneur and business magnate who founded companies like paypal, spacex, and tesla, revolutionizing industries with innovative technologies. * ancient mythological figure: persephone - the queen of the underworld and daughter of demeter, abducted by hades to become his wife and rulers of the underworld.": 0.6572769953051643,
    "Generate a one-sentence description for each of the following people: * historical figure: cleopatra * fictional character: sherlock holmes * real person: elon musk * ancient mythological figure: persephone": 0.36879432624113473,
    "Generate a one-sentence description for each of the following people.": 0.17460317460317462,
    "Generate a subjective title for the following email: subject: [your name] is leaving company (\"the goodbye note\")": 0.10606060606060608,
    "Is there anything i can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories? some options could be: * greek yogurt with nuts and fruit (around 900 calories, 20-25 grams of protein) * smoothie bowl with protein powder, banana, almond milk, and granola (around 800 calories, 30-35 grams of protein) * cottage cheese with avocado and whole-grain toast (around 900 calories, 25-30 grams of protein) * oatmeal with almond butter, banana, and milk (around 800 calories, 15-20 grams of protein) * whole-grain waffles with almond butter and sliced banana (around 700 calories, 12-15 grams of protein)": 0.08108108108108109
  },
  "avg_similarity_score": 0.13563849101581046,
  "metadata": {
    "prompt": "Come up with a series of tasks:\n1. What is the relation between the given pairs?\n2. Generate an appropriate subjective title for the following email\n3. Generate an appropriate subjective title for the following email: * \"can you explain why our project deadline was moved up again?\" 6.\n4. Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\n5. Generate a one-sentence description for each of the following people: * historical figure: cleopatra * fictional character: sherlock holmes * real person: elon musk * ancient mythological figure: persephone\n6.",
    "response": {
      "choices": [{
        "text": "Here are the tasks:\n\n1. What is the relation between the following pairs of words?\n\n* Apple - Banana\n* Car - Bike\n* House - Tree\n\n2. Generate a subjective title for the following email:\n\nSubject: [Your Name] is Leaving Company\n\n(\"The Goodbye Note\")\n\n3. Generate a subjective title for the following email:\n\nCan you explain why our project deadline was moved up again?\n\n(\"Unexpected Pushback\")\n\n4. Is there anything I can eat for breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\n\n(Hint: You can think of other protein-rich foods like Greek yogurt, avocado toast, or whole-grain waffles with nuts and seeds.)\n\n5. Generate a one-sentence description for each of the following people:\n\n* Historical figure: Cleopatra\n\t+ Cleopatra was the last active pharaoh of Ancient Egypt, known for her beauty, intelligence, and diplomatic skills.\n* Fictional character: Sherlock Holmes\n\t+ Sherlock Holmes is a brilliant detective created by Sir Arthur Conan Doyle, famous for his analytical mind and ability to solve complex mysteries.\n* Real person: Elon Musk\n\t+ Elon Musk is a pioneering entrepreneur and business magnate, known for his innovative ventures in electric cars, space exploration, and renewable energy.\n* Ancient mythological figure: Persephone\n\t+ Persephone was the queen of the underworld in Greek mythology, abducted by Hades and forced to spend part of each year with him, causing seasons to change.\n\n6. (No task provided, just awaiting more information!)"
      }]
    },
    "created_at": "2024-06-25 19:14:17.871848"
  },
  "request_idx": 4
}
```

## Summary
Each JSON object in the file represents a new instruction generated by the Llama-3 model, along with metadata and similarity scores to existing instructions. This setup allows for the iterative generation and refinement of a large instruction dataset, ensuring diversity and quality in the generated instructions.

